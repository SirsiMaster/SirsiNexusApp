#!/usr/bin/env python3
"""
SirsiNexus Enhanced Performance Test
Tests advanced analytics capabilities with TensorFlow, pandas, and Prophet
"""

import time
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import warnings

# Suppress TensorFlow warnings for cleaner output
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings('ignore')

def test_pandas_performance():
    """Test pandas DataFrame operations performance"""
    print("=== Testing Pandas Performance ===")
    start_time = time.time()
    
    # Create large dataset
    n_samples = 50000
    data = {
        'timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='1T'),
        'cpu_usage': np.random.normal(50, 15, n_samples),
        'memory_usage': np.random.normal(60, 20, n_samples),
        'network_io': np.random.exponential(100, n_samples),
        'disk_io': np.random.gamma(2, 50, n_samples),
        'response_time': np.random.lognormal(4, 0.5, n_samples)
    }
    df = pd.DataFrame(data)
    
    # Add some anomalies
    anomaly_indices = np.random.choice(n_samples, size=int(n_samples * 0.02), replace=False)
    df.loc[anomaly_indices, 'cpu_usage'] *= 3
    df.loc[anomaly_indices, 'response_time'] *= 5
    
    setup_time = time.time() - start_time
    
    # Performance operations
    start_ops = time.time()
    
    # Time-based operations (set index first)
    df.set_index('timestamp', inplace=True)
    
    # Statistical operations on numeric columns only
    numeric_cols = ['cpu_usage', 'memory_usage', 'network_io', 'disk_io', 'response_time']
    stats = df[numeric_cols].describe()
    correlations = df[numeric_cols].corr()
    rolling_means = df[numeric_cols].rolling(window=60).mean()
    
    # Time-based resampling operations
    hourly_avg = df.resample('1H').mean()
    daily_max = df.resample('1D').max()
    
    ops_time = time.time() - start_ops
    
    print(f"âœ“ Created DataFrame with {n_samples:,} samples in {setup_time:.3f}s")
    print(f"âœ“ Statistical operations completed in {ops_time:.3f}s")
    print(f"âœ“ Memory usage: {df.memory_usage().sum() / 1024 / 1024:.2f} MB")
    
    return df

def test_tensorflow_lstm_autoencoder():
    """Test TensorFlow LSTM autoencoder for anomaly detection"""
    print("\n=== Testing TensorFlow LSTM Autoencoder ===")
    
    try:
        import tensorflow as tf
        tf.get_logger().setLevel('ERROR')
        
        start_time = time.time()
        
        # Generate time series data
        n_samples = 1000
        sequence_length = 50
        
        # Normal pattern
        t = np.linspace(0, 100, n_samples)
        normal_data = np.sin(t) + 0.5 * np.sin(3*t) + 0.1 * np.random.normal(size=n_samples)
        
        # Add anomalies
        anomaly_indices = [200, 350, 600, 750]
        for idx in anomaly_indices:
            if idx < len(normal_data):
                normal_data[idx:idx+10] += np.random.normal(5, 1, min(10, len(normal_data)-idx))
        
        # Prepare sequences
        def create_sequences(data, seq_length):
            sequences = []
            for i in range(len(data) - seq_length + 1):
                sequences.append(data[i:i+seq_length])
            return np.array(sequences)
        
        sequences = create_sequences(normal_data, sequence_length)
        sequences = sequences.reshape(sequences.shape[0], sequence_length, 1)
        
        # Simple LSTM autoencoder
        model = tf.keras.Sequential([
            tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(sequence_length, 1)),
            tf.keras.layers.LSTM(16, return_sequences=False),
            tf.keras.layers.RepeatVector(sequence_length),
            tf.keras.layers.LSTM(16, return_sequences=True),
            tf.keras.layers.LSTM(32, return_sequences=True),
            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))
        ])
        
        model.compile(optimizer='adam', loss='mse')
        
        # Quick training (for demo)
        model.fit(sequences[:800], sequences[:800], epochs=5, batch_size=32, verbose=0)
        
        # Predict and calculate reconstruction errors
        predictions = model.predict(sequences, verbose=0)
        reconstruction_errors = np.mean((sequences - predictions) ** 2, axis=(1, 2))
        
        # Detect anomalies (simple threshold)
        threshold = np.percentile(reconstruction_errors, 95)
        detected_anomalies = np.where(reconstruction_errors > threshold)[0]
        
        training_time = time.time() - start_time
        
        print(f"âœ“ LSTM autoencoder trained in {training_time:.3f}s")
        print(f"âœ“ Processed {len(sequences):,} sequences")
        print(f"âœ“ Detected {len(detected_anomalies)} potential anomalies")
        print(f"âœ“ Model parameters: {model.count_params():,}")
        
        return reconstruction_errors
        
    except Exception as e:
        print(f"âš  TensorFlow test skipped: {str(e)}")
        return None

def test_prophet_forecasting():
    """Test Prophet for time series forecasting"""
    print("\n=== Testing Prophet Forecasting ===")
    
    try:
        from prophet import Prophet
        
        start_time = time.time()
        
        # Generate realistic time series data
        periods = 365 * 2  # 2 years of daily data
        dates = pd.date_range('2022-01-01', periods=periods, freq='D')
        
        # Simulate server metrics with trends, seasonality, and noise
        trend = np.linspace(100, 150, periods)  # Growing trend
        seasonal = 20 * np.sin(2 * np.pi * np.arange(periods) / 365)  # Yearly seasonality
        weekly = 10 * np.sin(2 * np.pi * np.arange(periods) / 7)  # Weekly seasonality
        noise = np.random.normal(0, 5, periods)
        
        values = trend + seasonal + weekly + noise
        
        # Create Prophet-compatible DataFrame
        df = pd.DataFrame({
            'ds': dates,
            'y': values
        })
        
        # Initialize and fit Prophet model
        model = Prophet(
            yearly_seasonality=True,
            weekly_seasonality=True,
            daily_seasonality=False,
            changepoint_prior_scale=0.05
        )
        
        model.fit(df)
        
        # Make future predictions
        future = model.make_future_dataframe(periods=90)  # 90 days ahead
        forecast = model.predict(future)
        
        training_time = time.time() - start_time
        
        # Calculate accuracy metrics on last 30 days
        test_period = 30
        actual = df['y'][-test_period:].values
        predicted = forecast['yhat'][-90:-60].values  # Corresponding predictions
        
        mae = np.mean(np.abs(actual - predicted))
        mape = np.mean(np.abs((actual - predicted) / actual)) * 100
        
        print(f"âœ“ Prophet model trained in {training_time:.3f}s")
        print(f"âœ“ Training data: {len(df)} daily observations")
        print(f"âœ“ Forecast horizon: 90 days")
        print(f"âœ“ MAE on test set: {mae:.2f}")
        print(f"âœ“ MAPE on test set: {mape:.2f}%")
        
        return forecast
        
    except Exception as e:
        print(f"âš  Prophet test skipped: {str(e)}")
        return None

def test_integrated_anomaly_detection(df):
    """Test integrated anomaly detection with multiple algorithms"""
    print("\n=== Testing Integrated Anomaly Detection ===")
    
    start_time = time.time()
    
    # Prepare features (excluding timestamp)
    feature_cols = ['cpu_usage', 'memory_usage', 'network_io', 'disk_io', 'response_time']
    X = df[feature_cols].fillna(df[feature_cols].mean())
    
    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Multiple anomaly detection algorithms
    algorithms = {
        'IsolationForest': IsolationForest(contamination=0.05, random_state=42),
    }
    
    results = {}
    
    for name, algo in algorithms.items():
        algo_start = time.time()
        
        # Fit and predict
        anomaly_labels = algo.fit_predict(X_scaled)
        anomaly_scores = algo.decision_function(X_scaled) if hasattr(algo, 'decision_function') else None
        
        # Count anomalies
        n_anomalies = np.sum(anomaly_labels == -1)
        
        algo_time = time.time() - algo_start
        
        results[name] = {
            'n_anomalies': n_anomalies,
            'processing_time': algo_time,
            'anomaly_rate': n_anomalies / len(X) * 100
        }
        
        print(f"âœ“ {name}: {n_anomalies:,} anomalies ({results[name]['anomaly_rate']:.2f}%) in {algo_time:.3f}s")
    
    total_time = time.time() - start_time
    print(f"âœ“ Total integrated detection time: {total_time:.3f}s")
    
    return results

def main():
    """Run all enhanced performance tests"""
    print("SirsiNexus Enhanced Analytics Performance Test")
    print("=" * 60)
    
    overall_start = time.time()
    
    # Test 1: Pandas performance
    df = test_pandas_performance()
    
    # Test 2: TensorFlow LSTM
    lstm_results = test_tensorflow_lstm_autoencoder()
    
    # Test 3: Prophet forecasting
    forecast_results = test_prophet_forecasting()
    
    # Test 4: Integrated anomaly detection
    anomaly_results = test_integrated_anomaly_detection(df)
    
    total_time = time.time() - overall_start
    
    print("\n" + "=" * 60)
    print("ðŸŽ‰ ALL ENHANCED TESTS COMPLETED!")
    print(f"Total execution time: {total_time:.3f}s")
    print("\nðŸ“Š Performance Summary:")
    print(f"- Processed {len(df):,} data points")
    print(f"- Multiple ML algorithms tested")
    print(f"- Advanced time series forecasting")
    print(f"- Deep learning anomaly detection")
    print("\nðŸš€ System ready for production-scale analytics!")

if __name__ == "__main__":
    main()
